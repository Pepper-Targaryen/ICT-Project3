{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,Dataset,TensorDataset\n",
    "# from utils import *\n",
    "\n",
    "from foolbox.models import PyTorchModel\n",
    "from foolbox.attacks import BoundaryAttack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.embed_num = 128\n",
    "        self.embed_dim = 5\n",
    "        self.class_num = 2\n",
    "        self.kernel_num = 100\n",
    "        self.kernel_sizes = [3,5,7,9]\n",
    "        #self.dropout = 0.3\n",
    "        self.static = True\n",
    "        \n",
    "class CNN_Text_dropout(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(CNN_Text_dropout, self).__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        V = args.embed_num\n",
    "        D = args.embed_dim\n",
    "        C = args.class_num\n",
    "        Ci = 1\n",
    "        Co = args.kernel_num\n",
    "        Ks = args.kernel_sizes\n",
    "\n",
    "        #self.embed = nn.Embedding(V, D)\n",
    "        # self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(in_channels=Ci, out_channels=Co, kernel_size=(K, D),stride=1) for K in Ks])\n",
    "        \n",
    "        #self.dropout = nn.Dropout(args.dropout)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, 256)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(64, C)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3)  # (N, Co, W)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "    \n",
    "    def weight_init(self):\n",
    "        for module in self.convs1:\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            torch.nn.init.constant_(module.bias, 0.1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.constant_(self.fc1.bias, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "\n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "        self.features = x\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout3(x)\n",
    "        logit = self.fc3(x)\n",
    "        #print('in the FP')\n",
    "        #print(x.shape)\n",
    "        #logit = self.fc3(self.relu(self.fc2()))\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_Text_dropout(\n",
      "  (convs1): ModuleList(\n",
      "    (0): Conv2d(1, 100, kernel_size=(3, 5), stride=(1, 1))\n",
      "    (1): Conv2d(1, 100, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (2): Conv2d(1, 100, kernel_size=(7, 5), stride=(1, 1))\n",
      "    (3): Conv2d(1, 100, kernel_size=(9, 5), stride=(1, 1))\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=400, out_features=256, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pepper/.conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/pepper/.conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/pepper/.conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/pepper/.conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/pepper/.conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_Text_dropout(\n",
       "  (convs1): ModuleList(\n",
       "    (0): Conv2d(1, 100, kernel_size=(3, 5), stride=(1, 1))\n",
       "    (1): Conv2d(1, 100, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (2): Conv2d(1, 100, kernel_size=(7, 5), stride=(1, 1))\n",
       "    (3): Conv2d(1, 100, kernel_size=(9, 5), stride=(1, 1))\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=400, out_features=256, bias=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (dropout3): Dropout(p=0.5, inplace=False)\n",
       "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train=np.load(\"./Data/sGrid/X_train.npy\")\n",
    "X_test=np.load(\"./Data/sGrid/X_test.npy\")\n",
    "X_vaild=np.load(\"./Data/sGrid/X_vaild.npy\")\n",
    "Y_train=np.load(\"./Data/sGrid/Y_train.npy\")\n",
    "Y_test=np.load(\"./Data/sGrid/Y_test.npy\")\n",
    "Y_vaild=np.load(\"./Data/sGrid/Y_vaild.npy\")\n",
    "\n",
    "torch.manual_seed(1)\n",
    "embedding = nn.Embedding(128, 5, max_norm = 1)\n",
    "\n",
    "Y_train = torch.from_numpy(Y_train)\n",
    "Y_test = torch.from_numpy(Y_test)\n",
    "Y_vaild = torch.from_numpy(Y_vaild)\n",
    "\n",
    "input = Variable(torch.from_numpy(X_train*128).long())\n",
    "X_train_embed = embedding(input)\n",
    "X_train_embed = X_train_embed.detach()\n",
    "\n",
    "input = Variable(torch.from_numpy(X_test*128).long())\n",
    "X_test_embed = embedding(input)\n",
    "X_test_embed = X_test_embed.detach()\n",
    "\n",
    "input = Variable(torch.from_numpy(X_vaild*128).long())\n",
    "X_vaild_embed = embedding(input)\n",
    "X_vaild_embed = X_vaild_embed.detach()\n",
    "\n",
    "dic = {}\n",
    "count = 0\n",
    "for i in range(X_train.shape[0]):\n",
    "    for j in range(400):\n",
    "        if chr(int(X_train[i,j]*128)) not in dic.keys():\n",
    "            dic[chr(int(X_train[i,j]*128))] = X_train_embed[i,j]\n",
    "\n",
    "symbol_dict = dic\n",
    "\n",
    "args = Args()\n",
    "\n",
    "net = CNN_Text_dropout(args).cuda()\n",
    "print(net)\n",
    "\n",
    "pretrained_dict = torch.load('Parameters/cnn_text_kernel3.5.7.9_128_embed_dropout.pkl').state_dict()\n",
    "model_dict = net.state_dict()\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "# 更新现有的model_dict\n",
    "model_dict.update(pretrained_dict)\n",
    "# 加载我们真正需要的state_dict\n",
    "net.load_state_dict(model_dict)\n",
    "\n",
    "\n",
    "batch_size = 500\n",
    "Train_data = Data.TensorDataset(X_train_embed, Y_train)\n",
    "Test_data = Data.TensorDataset(X_test_embed, Y_test)\n",
    "train_data = Data.DataLoader(dataset=Train_data, batch_size=batch_size, shuffle=False)\n",
    "test_data = Data.DataLoader(dataset=Test_data, batch_size=1, shuffle=False)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001, weight_decay=1e-9)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "attack_log_list = None\n",
    "attack_log_string_list = []\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PyTorchModel(net, (-1, 1), 2, )\n",
    "attack = BoundaryAttack(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 400, 5])\n"
     ]
    }
   ],
   "source": [
    "for data in test_data:\n",
    "    inputs, labels = data\n",
    "    print(inputs.shape)\n",
    "    break\n",
    "    labels = Variable(labels.cuda())\n",
    "    inputs=Variable(inputs.cuda(), requires_grad =True)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs.float())\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 0\n",
      "predicted class 0\n"
     ]
    }
   ],
   "source": [
    "test_index = 30\n",
    "url, label = X_test_embed[test_index].numpy(), int(Y_test[test_index].numpy()[0])\n",
    "print('label', label)\n",
    "print('predicted class', np.argmax(model.predictions(url)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run with verbose=True to see details\n",
      "Step 0: 1.99098e-02, stepsizes = 1.0e-02/1.0e-02: \n",
      "Step 1: 1.99098e-02, stepsizes = 1.0e-02/1.0e-02:  (took 0.02809 seconds)\n",
      "Step 2: 1.99098e-02, stepsizes = 1.0e-02/1.0e-02:  (took 0.02600 seconds)\n",
      "Step 3: 1.95136e-02, stepsizes = 1.0e-02/1.0e-02: d. reduced by 1.99% (3.9621e-04) (took 0.01498 seconds)\n",
      "Step 4: 1.95136e-02, stepsizes = 1.0e-02/1.0e-02:  (took 0.02596 seconds)\n",
      "Step 5: 1.95136e-02, stepsizes = 1.0e-02/1.0e-02:  (took 0.02590 seconds)\n",
      "Step 6: 1.95136e-02, stepsizes = 1.0e-02/1.0e-02:  (took 0.02750 seconds)\n",
      "Step 7: 1.95136e-02, stepsizes = 1.0e-02/1.0e-02:  (took 0.02593 seconds)\n",
      "Step 8: 1.95136e-02, stepsizes = 1.0e-02/1.0e-02:  (took 0.02616 seconds)\n",
      "Step 9: 1.95136e-02, stepsizes = 1.0e-02/1.0e-02:  (took 0.02560 seconds)\n",
      "Step 10: 1.95136e-02, stepsizes = 1.0e-02/1.0e-02:  (took 0.03997 seconds)\n",
      "Step 11: 1.95136e-02, stepsizes = 1.0e-02/1.0e-02:  (took 0.02562 seconds)\n",
      "Step 12: 1.95136e-02, stepsizes = 1.0e-02/1.0e-02:  (took 0.02570 seconds)\n",
      "Step 13: 1.95136e-02, stepsizes = 1.0e-02/1.0e-02:  (took 0.02561 seconds)\n",
      "Step 14: 1.95136e-02, stepsizes = 1.0e-02/1.0e-02:  (took 0.02537 seconds)\n",
      "Step 15: 1.95136e-02, stepsizes = 1.0e-02/1.0e-02:  (took 0.02562 seconds)\n",
      "Step 16: 1.95136e-02, stepsizes = 1.0e-02/1.0e-02:  (took 0.02571 seconds)\n",
      "Step 17: 1.95136e-02, stepsizes = 1.0e-02/1.0e-02:  (took 0.02543 seconds)\n",
      "Step 18: 1.95136e-02, stepsizes = 1.0e-02/1.0e-02:  (took 0.02583 seconds)\n",
      "Step 19: 1.95136e-02, stepsizes = 1.0e-02/1.0e-02:  (took 0.02566 seconds)\n",
      "Step 20: 1.95136e-02, stepsizes = 1.0e-02/6.7e-03:  (took 0.04340 seconds)\n",
      "Step 21: 1.92543e-02, stepsizes = 1.0e-02/6.7e-03: d. reduced by 1.33% (2.5931e-04) (took 0.01687 seconds)\n",
      "Step 22: 1.89984e-02, stepsizes = 1.0e-02/6.7e-03: d. reduced by 1.33% (2.5587e-04) (took 0.01789 seconds)\n",
      "Step 23: 1.89984e-02, stepsizes = 1.0e-02/6.7e-03:  (took 0.02618 seconds)\n",
      "Step 24: 1.89984e-02, stepsizes = 1.0e-02/6.7e-03:  (took 0.02610 seconds)\n",
      "Step 25: 1.89984e-02, stepsizes = 1.0e-02/6.7e-03:  (took 0.02579 seconds)\n",
      "Step 26: 1.89984e-02, stepsizes = 1.0e-02/6.7e-03:  (took 0.02583 seconds)\n",
      "Step 27: 1.89984e-02, stepsizes = 1.0e-02/6.7e-03:  (took 0.02559 seconds)\n",
      "Step 28: 1.89984e-02, stepsizes = 1.0e-02/6.7e-03:  (took 0.02564 seconds)\n",
      "Step 29: 1.89984e-02, stepsizes = 1.0e-02/6.7e-03:  (took 0.02561 seconds)\n",
      "Step 30: 1.89984e-02, stepsizes = 1.0e-02/6.7e-03:  (took 0.03861 seconds)\n",
      "Step 31: 1.89984e-02, stepsizes = 1.0e-02/6.7e-03:  (took 0.02604 seconds)\n",
      "Step 32: 1.89984e-02, stepsizes = 1.0e-02/6.7e-03:  (took 0.02616 seconds)\n",
      "Step 33: 1.89984e-02, stepsizes = 1.0e-02/6.7e-03:  (took 0.02628 seconds)\n",
      "Step 34: 1.87459e-02, stepsizes = 1.0e-02/6.7e-03: d. reduced by 1.33% (2.5247e-04) (took 0.01735 seconds)\n",
      "Step 35: 1.87459e-02, stepsizes = 1.0e-02/6.7e-03:  (took 0.02621 seconds)\n",
      "Step 36: 1.87459e-02, stepsizes = 1.0e-02/6.7e-03:  (took 0.02631 seconds)\n",
      "Step 37: 1.84968e-02, stepsizes = 1.0e-02/6.7e-03: d. reduced by 1.33% (2.4911e-04) (took 0.01604 seconds)\n",
      "Step 38: 1.84968e-02, stepsizes = 1.0e-02/6.7e-03:  (took 0.02597 seconds)\n",
      "Step 39: 1.82510e-02, stepsizes = 1.0e-02/6.7e-03: d. reduced by 1.33% (2.4580e-04) (took 0.01159 seconds)\n",
      "Step 40: 1.80085e-02, stepsizes = 1.0e-02/6.7e-03: d. reduced by 1.33% (2.4254e-04) (took 0.00594 seconds)\n",
      "Step 41: 1.77692e-02, stepsizes = 1.0e-02/6.7e-03: d. reduced by 1.33% (2.3931e-04) (took 0.00556 seconds)\n",
      "Step 42: 1.75331e-02, stepsizes = 1.0e-02/6.7e-03: d. reduced by 1.33% (2.3613e-04) (took 0.00840 seconds)\n",
      "Step 43: 1.73001e-02, stepsizes = 1.0e-02/6.7e-03: d. reduced by 1.33% (2.3299e-04) (took 0.02265 seconds)\n",
      "Step 44: 1.70702e-02, stepsizes = 1.0e-02/6.7e-03: d. reduced by 1.33% (2.2990e-04) (took 0.01360 seconds)\n",
      "Step 45: 1.70702e-02, stepsizes = 1.0e-02/6.7e-03:  (took 0.02593 seconds)\n",
      "Step 46: 1.68433e-02, stepsizes = 1.0e-02/6.7e-03: d. reduced by 1.33% (2.2684e-04) (took 0.00152 seconds)\n",
      "Step 47: 1.68433e-02, stepsizes = 1.0e-02/6.7e-03:  (took 0.02589 seconds)\n",
      "Step 48: 1.66195e-02, stepsizes = 1.0e-02/6.7e-03: d. reduced by 1.33% (2.2383e-04) (took 0.01103 seconds)\n",
      "Step 49: 1.66195e-02, stepsizes = 1.0e-02/6.7e-03:  (took 0.02627 seconds)\n",
      "Step 50: 1.66195e-02, stepsizes = 1.5e-02/1.0e-02:  (took 0.03390 seconds)\n",
      "Step 51: 1.66195e-02, stepsizes = 1.5e-02/1.0e-02:  (took 0.02548 seconds)\n",
      "Step 52: 1.66195e-02, stepsizes = 1.5e-02/1.0e-02:  (took 0.02539 seconds)\n",
      "Step 53: 1.62888e-02, stepsizes = 1.5e-02/1.0e-02: d. reduced by 1.99% (3.3073e-04) (took 0.00548 seconds)\n",
      "Step 54: 1.59646e-02, stepsizes = 1.5e-02/1.0e-02: d. reduced by 1.99% (3.2415e-04) (took 0.00258 seconds)\n",
      "Step 55: 1.56469e-02, stepsizes = 1.5e-02/1.0e-02: d. reduced by 1.99% (3.1770e-04) (took 0.00934 seconds)\n",
      "Step 56: 1.53355e-02, stepsizes = 1.5e-02/1.0e-02: d. reduced by 1.99% (3.1137e-04) (took 0.00850 seconds)\n",
      "Step 57: 1.50304e-02, stepsizes = 1.5e-02/1.0e-02: d. reduced by 1.99% (3.0518e-04) (took 0.00657 seconds)\n",
      "Step 58: 1.47313e-02, stepsizes = 1.5e-02/1.0e-02: d. reduced by 1.99% (2.9910e-04) (took 0.00569 seconds)\n",
      "Step 59: 1.44381e-02, stepsizes = 1.5e-02/1.0e-02: d. reduced by 1.99% (2.9315e-04) (took 0.00565 seconds)\n",
      "Step 60: 1.44381e-02, stepsizes = 1.5e-02/6.7e-03:  (took 0.04229 seconds)\n",
      "Step 61: 1.42462e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.9187e-04) (took 0.00249 seconds)\n",
      "Step 62: 1.40569e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.8932e-04) (took 0.00152 seconds)\n",
      "Step 63: 1.38701e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.8680e-04) (took 0.00146 seconds)\n",
      "Step 64: 1.36858e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.8432e-04) (took 0.00243 seconds)\n",
      "Step 65: 1.35039e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.8187e-04) (took 0.00148 seconds)\n",
      "Step 66: 1.33245e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.7945e-04) (took 0.01984 seconds)\n",
      "Step 67: 1.31474e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.7707e-04) (took 0.01655 seconds)\n",
      "Step 68: 1.29727e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.7471e-04) (took 0.00150 seconds)\n",
      "Step 69: 1.28003e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.7239e-04) (took 0.00260 seconds)\n",
      "Step 70: 1.26302e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.7010e-04) (took 0.00244 seconds)\n",
      "Step 71: 1.24624e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.6784e-04) (took 0.00237 seconds)\n",
      "Step 72: 1.22968e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.6561e-04) (took 0.00567 seconds)\n",
      "Step 73: 1.21333e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.6341e-04) (took 0.00361 seconds)\n",
      "Step 74: 1.19721e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.6124e-04) (took 0.00154 seconds)\n",
      "Step 75: 1.18130e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.5910e-04) (took 0.00558 seconds)\n",
      "Step 76: 1.16560e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.5698e-04) (took 0.00987 seconds)\n",
      "Step 77: 1.15011e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.5490e-04) (took 0.00156 seconds)\n",
      "Step 78: 1.13483e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.5284e-04) (took 0.00149 seconds)\n",
      "Step 79: 1.11975e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.5081e-04) (took 0.00149 seconds)\n",
      "Step 80: 1.10487e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.4880e-04) (took 0.00341 seconds)\n",
      "Step 81: 1.09019e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.4682e-04) (took 0.02352 seconds)\n",
      "Step 82: 1.07570e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.4487e-04) (took 0.00251 seconds)\n",
      "Step 83: 1.06140e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.4295e-04) (took 0.00140 seconds)\n",
      "Step 84: 1.04730e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.4105e-04) (took 0.00565 seconds)\n",
      "Step 85: 1.03338e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.3917e-04) (took 0.02107 seconds)\n",
      "Step 86: 1.03338e-02, stepsizes = 1.5e-02/6.7e-03:  (took 0.02547 seconds)\n",
      "Step 87: 1.01965e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.3732e-04) (took 0.02207 seconds)\n",
      "Step 88: 1.00610e-02, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.3550e-04) (took 0.00465 seconds)\n",
      "Step 89: 9.92729e-03, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.3370e-04) (took 0.00663 seconds)\n",
      "Step 90: 9.79537e-03, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.3192e-04) (took 0.00253 seconds)\n",
      "Step 91: 9.66520e-03, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.3017e-04) (took 0.00885 seconds)\n",
      "Step 92: 9.53676e-03, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.2844e-04) (took 0.00741 seconds)\n",
      "Step 93: 9.41003e-03, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.2673e-04) (took 0.00468 seconds)\n",
      "Step 94: 9.28498e-03, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.2505e-04) (took 0.01057 seconds)\n",
      "Step 95: 9.16159e-03, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.2339e-04) (took 0.00363 seconds)\n",
      "Step 96: 9.03984e-03, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.2175e-04) (took 0.00569 seconds)\n",
      "Step 97: 8.91971e-03, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.2013e-04) (took 0.01251 seconds)\n",
      "Step 98: 8.80118e-03, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.1853e-04) (took 0.00350 seconds)\n",
      "Step 99: 8.80118e-03, stepsizes = 1.5e-02/6.7e-03:  (took 0.02586 seconds)\n",
      "Step 100: 8.80118e-03, stepsizes = 1.5e-02/6.7e-03:  (took 0.03513 seconds)\n",
      "Step 101: 8.56882e-03, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.1540e-04) (took 0.52148 seconds)\n",
      "Step 102: 8.45495e-03, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.1387e-04) (took 0.01129 seconds)\n",
      "Step 103: 8.45495e-03, stepsizes = 1.5e-02/6.7e-03:  (took 0.01875 seconds)\n",
      "Step 104: 8.45495e-03, stepsizes = 1.5e-02/6.7e-03:  (took 0.04149 seconds)\n",
      "Step 105: 8.34259e-03, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.1236e-04) (took 0.04257 seconds)\n",
      "Step 106: 8.23173e-03, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.1086e-04) (took 0.03735 seconds)\n",
      "Step 107: 8.12234e-03, stepsizes = 1.5e-02/6.7e-03: d. reduced by 1.33% (1.0939e-04) (took 0.04229 seconds)\n",
      "Step 108: 8.12234e-03, stepsizes = 1.5e-02/6.7e-03:  (took 0.04395 seconds)\n",
      "Step 109: 8.12234e-03, stepsizes = 1.5e-02/6.7e-03:  (took 0.04182 seconds)\n",
      "Step 110: 8.01440e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 1.33% (1.0794e-04) (took 0.05359 seconds)\n",
      "Step 111: 7.94332e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 0.89% (7.1081e-05) (took 0.03639 seconds)\n",
      "Step 112: 7.87287e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 0.89% (7.0450e-05) (took 0.03780 seconds)\n",
      "Step 113: 7.80305e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 0.89% (6.9826e-05) (took 0.03886 seconds)\n",
      "Step 114: 7.73384e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 0.89% (6.9206e-05) (took 0.03816 seconds)\n",
      "Step 115: 7.66525e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 0.89% (6.8592e-05) (took 0.03866 seconds)\n",
      "Step 116: 7.59726e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 0.89% (6.7984e-05) (took 0.03846 seconds)\n",
      "Step 117: 7.52988e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 0.89% (6.7381e-05) (took 0.03789 seconds)\n",
      "Step 118: 7.46310e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 0.89% (6.6784e-05) (took 0.03747 seconds)\n",
      "Step 119: 7.39691e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 0.89% (6.6191e-05) (took 0.03943 seconds)\n",
      "Step 120: 7.33130e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 0.89% (6.5604e-05) (took 0.04505 seconds)\n",
      "Step 121: 7.26628e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 0.89% (6.5022e-05) (took 0.03659 seconds)\n",
      "Step 122: 7.20183e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 0.89% (6.4446e-05) (took 0.03943 seconds)\n",
      "Step 123: 7.13796e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 0.89% (6.3874e-05) (took 0.03785 seconds)\n",
      "Step 124: 7.07465e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 0.89% (6.3308e-05) (took 0.03696 seconds)\n",
      "Step 125: 7.01191e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 0.89% (6.2746e-05) (took 0.03597 seconds)\n",
      "Step 126: 6.94972e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 0.89% (6.2190e-05) (took 0.03841 seconds)\n",
      "Step 127: 6.88808e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 0.89% (6.1638e-05) (took 0.03767 seconds)\n",
      "Step 128: 6.82699e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 0.89% (6.1091e-05) (took 0.03722 seconds)\n",
      "Step 129: 6.76644e-03, stepsizes = 1.5e-02/4.4e-03: d. reduced by 0.89% (6.0549e-05) (took 0.03837 seconds)\n",
      "Step 130: 6.70643e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 0.89% (6.0012e-05) (took 0.05456 seconds)\n",
      "Step 131: 6.61730e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 1.33% (8.9121e-05) (took 0.03810 seconds)\n",
      "Step 132: 6.52937e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 1.33% (8.7937e-05) (took 0.03707 seconds)\n",
      "Step 133: 6.44260e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 1.33% (8.6768e-05) (took 0.03851 seconds)\n",
      "Step 134: 6.35699e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 1.33% (8.5615e-05) (took 0.03931 seconds)\n",
      "Step 135: 6.27251e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 1.33% (8.4477e-05) (took 0.03795 seconds)\n",
      "Step 136: 6.18915e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 1.33% (8.3355e-05) (took 0.03820 seconds)\n",
      "Step 137: 6.10691e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 1.33% (8.2247e-05) (took 0.03795 seconds)\n",
      "Step 138: 6.02575e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 1.33% (8.1154e-05) (took 0.03817 seconds)\n",
      "Step 139: 5.94568e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 1.33% (8.0076e-05) (took 0.04226 seconds)\n",
      "Step 140: 5.86667e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 1.33% (7.9011e-05) (took 0.04280 seconds)\n",
      "Step 141: 5.78870e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 1.33% (7.7961e-05) (took 0.03797 seconds)\n",
      "Step 142: 5.71178e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 1.33% (7.6925e-05) (took 0.03823 seconds)\n",
      "Step 143: 5.63588e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 1.33% (7.5903e-05) (took 0.04066 seconds)\n",
      "Step 144: 5.56098e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 1.33% (7.4895e-05) (took 0.03836 seconds)\n",
      "Step 145: 5.48708e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 1.33% (7.3899e-05) (took 0.03742 seconds)\n",
      "Step 146: 5.41416e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 1.33% (7.2917e-05) (took 0.03738 seconds)\n",
      "Step 147: 5.34222e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 1.33% (7.1948e-05) (took 0.03681 seconds)\n",
      "Step 148: 5.34222e-03, stepsizes = 2.2e-02/6.7e-03:  (took 0.04430 seconds)\n",
      "Step 149: 5.27122e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 1.33% (7.0992e-05) (took 0.03619 seconds)\n",
      "Step 150: 5.20118e-03, stepsizes = 2.2e-02/6.7e-03: d. reduced by 1.33% (7.0049e-05) (took 0.04483 seconds)\n",
      "adversarial class 1\n"
     ]
    }
   ],
   "source": [
    "adversarial = attack(url, label, iterations=150)\n",
    "\n",
    "print('adversarial class', np.argmax(model.predictions(adversarial)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tensor_to_Log(symbol_dict, input_tensor, find_nearest=True, end_filter=True):\n",
    "    count = 0\n",
    "    string = ''\n",
    "    input_tensor = input_tensor.squeeze() \n",
    "    for i in range(input_tensor.shape[0]):  \n",
    "        if not find_nearest:\n",
    "            for ele in symbol_dict:\n",
    "                # precision problem\n",
    "\n",
    "                if torch.sum(abs(symbol_dict[ele] - input_tensor[i].cpu()) > 1e-5).item() == 0:\n",
    "                    #if ele != ' ':\n",
    "                    string += ele\n",
    "                    count += 1\n",
    "                    #print(ele)\n",
    "                    break      \n",
    "        else:\n",
    "            # find the nearest vector\n",
    "            error = 1 # should decrease the error\n",
    "            good_symbol = ''\n",
    "            for symbol in symbol_dict:\n",
    "                temp = torch.sum((symbol_dict[symbol] - input_tensor[i].cpu()) ** 2)\n",
    "                if error > temp:\n",
    "                    error = temp\n",
    "                    good_symbol = symbol\n",
    "                    #print(error)\n",
    "            count += 1       \n",
    "            string += good_symbol\n",
    "            \n",
    "            # filter other characters at the end\n",
    "            if end_filter and good_symbol == '\\x00':\n",
    "                for _ in range(400 - count):\n",
    "                    string += '\\x00'\n",
    "                break\n",
    "    return string\n",
    "\n",
    "def Log_to_Tensor(symbol_dict, log):\n",
    "    Tensor = torch.zeros([400,5])\n",
    "    for i in range(400):\n",
    "        Tensor[i] = symbol_dict['\\x00']\n",
    "    for i, e in enumerate(log):\n",
    "        for ele in symbol_dict:\n",
    "            if e == ele:\n",
    "                Tensor[i] = symbol_dict[ele]\n",
    "    return Tensor\n",
    "\n",
    "#def v2str(symbol_dict, vectors):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40934753,  0.5099589 , -0.6994622 , -0.27492952,  0.08684997],\n",
       "       [-0.12354521,  0.48398593, -0.40627903,  0.6420066 , -0.41624388],\n",
       "       [ 0.2864714 , -0.626523  , -0.25683284,  0.3233661 , -0.59571314],\n",
       "       ...,\n",
       "       [-0.6270978 , -0.30838355, -0.26881954, -0.6615804 , -0.04117382],\n",
       "       [-0.6270978 , -0.30838355, -0.26881954, -0.6615804 , -0.04117382],\n",
       "       [-0.6270978 , -0.30838355, -0.26881954, -0.6615804 , -0.04117382]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Log_to_Tensor(symbol_dict, '/tienda1/imagenes/1.gif').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original log: /tienda1/imagenes/1.gif\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "adversarial log: /tienQa1/ima1ene3/1.gif\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000_\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000x\u0000x\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000_\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n"
     ]
    }
   ],
   "source": [
    "print('original log:', Tensor_to_Log(symbol_dict, torch.from_numpy(url)))\n",
    "print('adversarial log:', Tensor_to_Log(symbol_dict, torch.from_numpy(adversarial)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original log: /tienda1/publico/anadir.jsp?id=2&nombre=Vino+Rioja&precio=85&cantidad=88&B1=A%F1adir+al+carrito\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "adversarial log: btienda1YiLagenes/2.gif\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n"
     ]
    }
   ],
   "source": [
    "print('original log:', Tensor_to_Log(symbol_dict, torch.from_numpy(url)))\n",
    "print('adversarial log:', Tensor_to_Log(symbol_dict, torch.from_numpy(adversarial)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def boundary_attack(nearest_starting=True, n_test = 100, max_iteration = 150, try_time=1):\n",
    "    test_set = X_test_embed\n",
    "    if nearest_starting:\n",
    "        test_set = X_test_nearest_tensor\n",
    "    \n",
    "    n_success = 0\n",
    "    n_total = 0\n",
    "    iterations = []\n",
    "\n",
    "    file = open(f'./Data/boundary_attack_unfixed_iteration_nearest_starting_max_{max_iteration}_test_{n_test}_{try_time}.txt',\"w\") \n",
    "    for i in range(n_test):\n",
    "        url, label = test_set[i+(try_time-1)*2].numpy(), int(Y_test[i+(try_time-1)*2].numpy()[0])\n",
    "        prediction = np.argmax(model.predictions(url))\n",
    "\n",
    "        if label == 0 and prediction == 0:\n",
    "            n_total += 1\n",
    "            good_adversarial = None\n",
    "            good_iteration = 0\n",
    "            for iteration in range(max_iteration+1):\n",
    "                adversarial = attack(url, label, iterations=iteration)\n",
    "\n",
    "                # adversarial log\n",
    "                str_adversarial = Tensor_to_Log(symbol_dict, torch.from_numpy(adversarial))\n",
    "                # need to change the adversarial string back to the tensor\n",
    "                prediction = np.argmax(model.predictions(Log_to_Tensor(symbol_dict, str_adversarial).numpy()))\n",
    "                if prediction == 1:\n",
    "                    good_iteration = iteration\n",
    "                    good_adversarial = adversarial\n",
    "\n",
    "            if not good_adversarial is None:\n",
    "                n_success += 1\n",
    "                iterations.append(good_iteration)\n",
    "                # original log\n",
    "                file.write(Tensor_to_Log(symbol_dict, torch.from_numpy(url)))\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "                # adversarial log\n",
    "                file.write(Tensor_to_Log(symbol_dict, torch.from_numpy(good_adversarial)))\n",
    "                file.write(\"\\n\\n\")          \n",
    "\n",
    "    file.close()\n",
    "    return n_success, n_total, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-e3c60717b358>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn_success\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboundary_attack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-583ec3942d75>\u001b[0m in \u001b[0;36mboundary_attack\u001b[0;34m(nearest_starting, n_test, max_iteration, try_time)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_embed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnearest_starting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnearest_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mn_success\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-0184f9388c4e>\u001b[0m in \u001b[0;36mnearest_tensor\u001b[0;34m(X_test_embed)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstr_X_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_test_embed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mstr_X_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor_to_Log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdict_nearest_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_nearest_adversial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_X_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_attack_log_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_similarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-847248e5b9a4>\u001b[0m in \u001b[0;36mTensor_to_Log\u001b[0;34m(symbol_dict, input_tensor, find_nearest, end_filter)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mgood_symbol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msymbol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msymbol_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_success, n_total, iterations = boundary_attack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "print(f'Success Rate: {n_success * 100.0 / n_total:.4}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "_ = plt.hist(iterations, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(f\"# of iterations with {len(iterations)} successful attacks\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./Data/boundary_attack_iterations_100_samples_2.npy', np.array(iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "test = np.load('./Data/boundary_attack_iterations_100_samples.npy')\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate:  33.06%\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "file = open('./Data/boundary_attack_iteration_50_test_10000.txt',\"r\") \n",
    "texts = file.readlines()\n",
    "file.close()\n",
    "\n",
    "count = 0\n",
    "for i in range(2, len(texts), 4):\n",
    "    if int(texts[i]) == 1:\n",
    "        count += 1\n",
    "print(f\"Success rate: {count * 1.0 / len(texts) * 4 * 100 : .4}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8950.0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def f():\n",
    "    dict_attack_log = []\n",
    "    for i in tqdm.tqdm_notebook(range(len(X_train_embed))):\n",
    "        url, label = X_train_embed[i], int(Y_train[i].numpy()[0])\n",
    "        if label == 1:\n",
    "            log = Tensor_to_Log(symbol_dict, url)\n",
    "            dict_attack_log.append(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fin(i):\n",
    "    tensor, label = X_train_embed[i], int(Y_train[i].numpy()[0])\n",
    "    if label == 1:\n",
    "        log = Tensor_to_Log(symbol_dict, tensor)\n",
    "        return log, tensor\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ecf7651a5f403088ede83fbdc470e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=42468), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-91:\n",
      "Process ForkPoolWorker-93:\n",
      "Process ForkPoolWorker-94:\n",
      "Process ForkPoolWorker-96:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-89:\n",
      "Process ForkPoolWorker-90:\n",
      "Process ForkPoolWorker-95:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-92:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/pepper/.conda/envs/torch/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "pool = Pool(8)\n",
    "\n",
    "dict_attack_log_tensor = {}\n",
    "for i,ret_msg in tqdm.tqdm_notebook(enumerate(pool.imap_unordered(fin,range(len(X_train_embed)))),total = len(X_train_embed)):\n",
    "    #print(i)\n",
    "    if ret_msg is not None:\n",
    "        log, tensor = ret_msg\n",
    "        dict_attack_log_tensor[log] = tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17268"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_attack_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "def find_nearest_adversial(origins, candidate, distance_metric):\n",
    "    dic = {}\n",
    "    assert (len(candidate) > 0),\"There should be at least one candidate\"\n",
    "    for i in tqdm.tqdm_notebook(origins):\n",
    "        min_norm = None\n",
    "        min_candidate = None\n",
    "        for j in candidate:\n",
    "            if min_norm is None:\n",
    "                min_norm = distance_metric(i,j)\n",
    "                min_candidate = j\n",
    "            else:\n",
    "                new_distance = distance_metric(i,j)\n",
    "                min_norm, min_candidate = (min_norm, min_candidate) if new_distance > min_norm else (new_distance, j)\n",
    "        dic[i] = min_candidate\n",
    "    return dic\n",
    "def str_similarity(str_a, str_b):\n",
    "    #str_a = Tensor_to_Log(symbol_dict, tensor_a)\n",
    "    #str_b = Tensor_to_Log(symbol_dict, tensor_b)\n",
    "    return difflib.SequenceMatcher(a=str_a, b=str_b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_tensor(X_test_embed):\n",
    "    str_X_test = []\n",
    "    for tensor in tqdm.tqdm_notebook(X_test_embed):\n",
    "        str_X_test.append(Tensor_to_Log(symbol_dict, tensor))\n",
    "    dict_nearest_str = find_nearest_adversial(str_X_test, dict_attack_log_tensor.keys(), str_similarity)\n",
    "\n",
    "    X_test_nearest_tensor = []\n",
    "    for log in str_X_test:\n",
    "        X_test_nearest_tensor.append(dict_attack_log_tensor[dict_nearest_str[log]])\n",
    "    return X_test_nearest_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cecab65e839c42ed873737923485406b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15167), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test_nearest_tensor = nearest_tensor(X_test_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42468, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17268., dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12354521,  0.48398593, -0.40627903,  0.6420066 , -0.41624388],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.27464807,  0.49253538, -0.4223426 ,  0.44162118, -0.27377424],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversarial[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
